{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica #6: Bayes Ingenuo\n",
        "\n",
        "Profesor: Luis Norberto Zúñiga Morales, Universidad Iberoamericana Ciudad de México\n",
        "\n",
        "Instrucciones:\n",
        "\n",
        "- Realizar cada una de las actividades propuestas en este libro de Google Colab.\n",
        "- Se puede realizar en equipos de 2 personas.\n",
        "- Para la entrega de sus respuestas, es importante que la celda en cuestión se encuentre ejecutada para mostrar su resultados.\n",
        "- Descargar su libro con terminación IPYNB y subirlo en la actividad correspondiente de Brigthspace.\n"
      ],
      "metadata": {
        "id": "GIVaDCQYwPtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos las librerías que usaremos para nuestro programa."
      ],
      "metadata": {
        "id": "pnUxKBA9Nupe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nombres:"
      ],
      "metadata": {
        "id": "-E-4-GkeIZmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from math import pi\n",
        "from math import exp"
      ],
      "metadata": {
        "id": "Hu26zSuhNuHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primer paso: preparar los datos para el algoritmo"
      ],
      "metadata": {
        "id": "XR0i9VBn2T_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo primero que hacemos es una función que nos separa los datos en clases. Recurden que esto es importante ya que NB funciona alrededor de los ejemplos observados en cada clase de nuestro conjunto de datos. En esta práctica, cada dato tiene la forma:\n",
        "\n",
        "[$x_1$, $x_2$, clase]\n",
        "\n",
        "Además, vamos a guardar cada punto del conjunto de datos en la variable `dataset`, que es una lista de listas."
      ],
      "metadata": {
        "id": "NluE10_-SbzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [[3.393533211,2.331273381,0],\n",
        "\t[3.110073483,1.781539638,0],\n",
        "\t[1.343808831,3.368360954,0],\n",
        "\t[3.582294042,4.67917911,0],\n",
        "\t[2.280362439,2.866990263,0],\n",
        "\t[7.423436942,4.696522875,1],\n",
        "\t[5.745051997,3.533989803,1],\n",
        "\t[9.172168622,2.511101045,1],\n",
        "\t[7.792783481,3.424088941,1],\n",
        "\t[7.939820817,0.791637231,1]]"
      ],
      "metadata": {
        "id": "2K2q4v0T0MxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(3 puntos)* Vamos a crear una función especial cuyo objetivo es separar los datos en dos clases: los de la clase `0` y los de la clase `1`. En el celda de código de abajo, crear una función que tome como argumentos el dataset (una lista de listas) y regrese un diccionario con la siguiente estructura:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "{ 0: [[x1, x2, clase], [x1, x2, clase],...]\n",
        "1: [[x1, x2, clase], [x1, x2, clase],...]\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "yXRZN6IX0G0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separar_clases(dataset):\n",
        "\tseparated = dict()\n",
        "\t# to-do: terminar el código\n",
        "\treturn separated"
      ],
      "metadata": {
        "id": "x-ysdz3GSWrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como guía, revisen el diccionario de abajo que resulta al mandar llamar la función `separar_clases`."
      ],
      "metadata": {
        "id": "q1aKjbV52l_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(separar_clases(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8hYhEuy5TXd",
        "outputId": "f3e99a24-c744-4396-e21d-f3c41a852080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [[3.393533211, 2.331273381, 0], [3.110073483, 1.781539638, 0], [1.343808831, 3.368360954, 0], [3.582294042, 4.67917911, 0], [2.280362439, 2.866990263, 0]], 1: [[7.423436942, 4.696522875, 1], [5.745051997, 3.533989803, 1], [9.172168622, 2.511101045, 1], [7.792783481, 3.424088941, 1], [7.939820817, 0.791637231, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segundo paso: estadísticas del modelo"
      ],
      "metadata": {
        "id": "D0yQobVi2tBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El segundo paso consiste en determinar unas estadísticas para nuestro modelo. En particular, vamos a suponer que los datos siguien una distribución $N∼(\\mu,\\sigma^2)$, por lo que necesitamos estimar los parámetros correspondiente usando nuestro conjunto de datos con las siguientes fórmulas:\n",
        "\n",
        "* $μ=\\frac{1}{N}\\sum_{i=1}^N x_i$\n",
        "\n",
        "* $\\sigma = \\sqrt{\\frac{\\sum_{i=1}^N x_i – \\mu(x)^2}{N-1}}$\n",
        "\n",
        "donde $N$ indica el número de elementos del conjunto de datos."
      ],
      "metadata": {
        "id": "Z_kGwa2STJMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(4 puntos)* Crear dos funciones que calculen la media y la desviación estándar de un conjunto de puntos. Cada función debe regresar un escalar. Más adelante las vamos a usar."
      ],
      "metadata": {
        "id": "3lkJQJ0E3ntK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula la media de una lista de numeros\n",
        "def media(numeros):\n",
        "\treturn # to-do: terminar el código"
      ],
      "metadata": {
        "id": "CJ0rnF2_TLqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula la desviacion estandar de una lista de numeros\n",
        "def stdev(numeros):\n",
        "\treturn # to-do: terminar el código"
      ],
      "metadata": {
        "id": "--D5niL_TQmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesitamos determinar la media y desviación estándar de cada característica que modela nuestros datos, es decir, si la primera características es representada por $x_1$ y tenemos 5 datos, necesitamos determinar la media y desviación estándar de los puntos\n",
        "$$\\{x_1^{(1)}, x_1^{(2)}, x_1^{(3)}, x_1^{(4)}, x_1^{(5)}\\}$$\n",
        "\n",
        "lo cual se puede interpretar como calcular las estadísticas a nivel columna en nuestro conjunto de datos."
      ],
      "metadata": {
        "id": "NuN1fmMfz58J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(2 puntos)* Crear una función llamada `resumen_dataset` que pida como argumento el `dataset` y calcule la media, desviación estándar y conteo de elementos en cada columna. Como resultado, debe regresar un lista de tuplas, donde cada tupla corresponde a la media, desviación estándar y conteo de elementos de cada columna:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jfX-v6SG6pdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calcula la media, desviacion estandar y el conteo de cada columna del dataset\n",
        "def resumen_dataset(dataset):\n",
        "\t# to-do: terminar el código\n",
        "\treturn resultados"
      ],
      "metadata": {
        "id": "fQzFBxf2TdHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resumen_dataset(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXc3yywK0P7U",
        "outputId": "76b291cb-8354-4014-fd33-7ed1985a68a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a organizar el código un poco más usando todo lo anterior al crear una función que solicite la lista con las estadísticas anteriores y la vamos a meter en un diccionario para su uso posterior, con la particularidad de tener toda la información necesaria en un formato adecuado. Noten que los estadísticas son por clase (0 y 1) y para cada columna de características."
      ],
      "metadata": {
        "id": "TmIy0Tau0ZHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar el dataset por clase y calcular las estadisticas para cada fila\n",
        "def resumen_por_clase(dataset):\n",
        "  # separamos el dataset por clases, 0 y 1\n",
        "\tseparated = separar_clases(dataset)\n",
        "\tsummaries = dict()\n",
        "  # se calculan las estadisticas para cada subconjunto\n",
        "\tfor class_value, rows in separated.items(): # itera sobre la llave (clase) y valor (vector de datos)\n",
        "\t\tsummaries[class_value] = resumen_dataset(rows)\n",
        "\treturn summaries"
      ],
      "metadata": {
        "id": "MEj7GBPdxt1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resumen_por_clase(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjix2irS0vMX",
        "outputId": "31d9c1d1-4f33-4abf-89fc-255a751a85e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [(2.7420144012, 0.9265683289298018, 5), (3.0054686692, 1.1073295894898725, 5)], 1: [(7.6146523718, 1.2344321550313704, 5), (2.9914679790000003, 1.4541931384601618, 5)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tercer paso: construcción de la función de probabilidad del modelo"
      ],
      "metadata": {
        "id": "lKETurdP_hvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente fase consiste en construir nuestra función de probabilidad para cade clase con la media y varianza que determinamos en los puntos anteriores. Vamos a programar la función normal:\n",
        "\n",
        "$$f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp({-\\frac{(x-\\mu)^2}{(2 * sigma^2)}})$$"
      ],
      "metadata": {
        "id": "q7uC5U7h1B57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la funcion de distribucion gaussiana\n",
        "def calcular_probabilidad(x, media, stdev):\n",
        "\texponente = exp(-((x-media)**2 / (2 * stdev**2 )))\n",
        "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponente"
      ],
      "metadata": {
        "id": "PdR9IWuXT2-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para calcular las probabilidades de cada clase, vamos a utilizar todo lo que hemos hecho hasta el momento. La probabilidad de que un dato pertenezca a una clase se determina por:\n",
        "\n",
        "$$P(clase|x) = P(x|clase) * P(clase)$$\n",
        "\n",
        "Utilizando el supuesto ingenuo (independencia entre las características que modelan los datos), tenemos que:\n",
        "\n",
        "$$P(clase=0|x_1,x_2) = P(x_1|clase=0)  P(x_2|clase=0)  P(clase=0)$$\n",
        "\n",
        "Es decir, necesitamos trabajar cada columna por separado ($x_1$ y $x_2$) y, además, para cada clase (0 y 1).\n",
        "\n",
        "1. Primero, el número total de datos de entrenamiento se calcula a partir del conteo almacenado en las estadísticas de resumen para cada clase. Esto se usa en el cálculo de la probabilidad de una clase dada o $P(clase)$ como la proporción de filas con una clase dada de todas las filas en los datos de entrenamiento.\n",
        "\n",
        "2. A continuación, se calculan las probabilidades para cada valor de entrada en la fila utilizando la función de densidad de probabilidad gaussiana y las estadísticas para esa columna y de esa clase. Las probabilidades se multiplican juntas a medida que se acumulan.\n",
        "\n",
        "3. Este proceso se repite para cada clase en el conjunto de datos.\n",
        "\n",
        "4. Finalmente, se devuelve un diccionario de probabilidades con una entrada para cada clase."
      ],
      "metadata": {
        "id": "IT5HMczP2Cmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular las probabilidad de cada clase para cada fila del dataset\n",
        "def calcular_probabilidades_clase(resumen, fila):\n",
        "\tfilas_totales = sum([resumen[label][0][2] for label in resumen])\n",
        "\t# print([resumen[label][0][2] for label in resumen])\n",
        "\t# print('filas totales', filas_totales)\n",
        "\tprobs = dict() #diccionario que guarda las probabilidades\n",
        "\tfor class_value, class_summaries in resumen.items(): #llave: valor de la clase, valor: estadisticos de cada columna\n",
        "\t\tprobs[class_value] = resumen[class_value][0][2]/float(filas_totales) #5/10, P(CLASE=0)\n",
        "\t\tfor i in range(len(class_summaries)): # itera sobre x_1 y x_2\n",
        "\t\t\tmedia, stdev, _ = class_summaries[i] # extrae los estadisticos\n",
        "\t\t\tprobs[class_value] *= calcular_probabilidad(fila[i], media, stdev) # i=0 , calcula la prob. gaussiana P(x_1|clase=0)\n",
        "\treturn probs"
      ],
      "metadata": {
        "id": "yFE5JbDpURzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAKjdm8I48a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0be3e4c-fad1-4178-8d36-325c51a04679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.03891229878967956, 1: 4.025712313269427e-05}\n"
          ]
        }
      ],
      "source": [
        "# mostramos los resultados\n",
        "resumen = resumen_por_clase(dataset)\n",
        "# print('resumen:',resumen)\n",
        "probs = calcular_probabilidades_clase(resumen, dataset[1])\n",
        "print(probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cuarto paso: realizar las predicciones con el modelo entrenado"
      ],
      "metadata": {
        "id": "zs6jHOSiHL5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear una función para predecir la clase de un nuevo dato:"
      ],
      "metadata": {
        "id": "sGvyZ198E2wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction - highest probability is the prediction\n",
        "def predict(resumen, nuevo_dato):\n",
        "    probs = calcular_probabilidades_clase(resumen, nuevo_dato)\n",
        "    best_label, best_prob = None, -1\n",
        "    for class_val, prob in probs.items():\n",
        "        if best_label is None or prob > best_prob:\n",
        "            best_prob = prob\n",
        "            best_label = class_val\n",
        "    return best_label"
      ],
      "metadata": {
        "id": "o43v_KHBDZ3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(resumen_por_clase(dataset), dataset[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6dHBwkGDsHx",
        "outputId": "8b7d3577-d08d-4698-f69e-fe739d2abc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(2 puntos)*: Crear una función que, dado un conjunto de puntos, determine su clase usando la función `predict()` anterior."
      ],
      "metadata": {
        "id": "62oiJfrqGFHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mass_predict(resumen, datos):\n",
        "  # to-do: terminar código"
      ],
      "metadata": {
        "id": "he0DEfdzG9gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actividad"
      ],
      "metadata": {
        "id": "fWRJymlqH7vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(8 puntos)*: Usar el código anterior para entrenar un clasificador Bayesiano Ingenuo con el famoso dataset de Iris. Verificar que se haga lo siguiente:\n",
        "\n",
        "- Descargar o importar el dataset (como les sea más conveniente). Explicar en una celda de código sobre qué trata el conjunto de datos.\n",
        "- Ajustar los datos al formato que maneja el código en esta práctica.\n",
        "- Separar los datos en conjuntos de prueba y entrenamiento. Entrenar el modelo con la partición de entrenamiento, y realizar las predicciones con la partición de prueba.\n",
        "- Evaluar su modelo usando la medida F1, ya sea que ustedes la definan o puede utilizar cualquier otra definida en una librería.\n",
        "- Reportar y explicar sus hallazgos."
      ],
      "metadata": {
        "id": "1omh0wSS8P2l"
      }
    }
  ]
}